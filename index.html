<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Yunpeng Bai">
  <meta name="description" content="Yunpeng Bai's Homepage">
  <meta name="keywords" content="Yunpeng Bai,白云鹏,homepage,主页,computer vision,Tsinghua,image generation, neural rendering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Yunpeng Bai (白云鹏)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yunpeng Bai (白云鹏)</name>
              </p>
              <p style="text-align:center">
                Email: byp215[at]gmail.com &nbsp; &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=OJNkfm8AAAAJ&hl=en">Google Scholar</a> &nbsp; &nbsp;&nbsp;&nbsp;<a href="https://github.com/bbaaii">Github</a>
              </p>
              <p>
                I am Yunpeng Bai, a Ph.D. student in Computer Science at UT Austin, advised by <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>. 
                My interests lie in using computer algorithms to create visually striking content, with a recent focus on 3D foundational models and 3D visual content generation. 
                I previously graduated with a master’s degree from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, where I was an outstanding graduate. 
                During my time at Tsinghua, I had the opportunity to intern extensively at <a
                href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a>, where I gained significant practical experience 
                and had the privilege of working with several distinguished researchers, including <a href="https://xuanwangvc.github.io/">Xuan Wang</a>, <a href="https://yzhang2016.github.io/">Yong Zhang</a>, <a href="https://xinntao.github.io/">Xintao Wang</a> and <a href="https://yanpei.me/">Yan-Pei Cao</a>.
                I also work closely with Prof. <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a>.
                
                
                <!-- I have graduated from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> with a Master's degree in Computer Technology. My research interests include neural rendering, 2D/3D content generation and neural representation. During my time at Tsinghua, I had the opportunity to intern at the <a
                href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a>, where I had the privilege of working with <a href="https://xuanwangvc.github.io/">Xuan Wang</a>, <a href="https://yzhang2016.github.io/">Yong Zhang</a>, <a href="https://xinntao.github.io/">Xintao Wang</a> and <a href="https://yanpei.me/">Yan-Pei Cao</a>. I also work closely with Prof. <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a>. Before joining Tsinghua University, I received B.E. in Computer Science from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>.  -->
              </p>
              <!-- <p>
                <strong>
                  I am actively looking for a Ph.D. position in computer graphics/vision for 2024 fall.
                </strong>
              </p> -->
		    
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/yunpeng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yunpeng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>





          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images/dreamdiffusion.jpg' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment</papertitle>
              <br>
              <strong>Yunpeng Bai</strong>, <a href="https://xinntao.github.io/">Xintao Wang</a>,
               <a href="https://yanpei.me/">Yan-Pei Cao</a>,
               <a href="https://geyixiao.com/">Yixiao Ge</a>, 
               <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>,
               <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2024,
              <br>
              <a href="https://arxiv.org/abs/2306.16934">[PDF]</a>
              <a href="https://github.com/bbaaii/DreamDiffusion">[Code]</a> 
              <br>
              <p>This paper introduces DreamDiffusion, the first method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text.</p>
          </td>
      </tr> 


      <tr></tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" >
              <img src='images/24-eccv-mesongs.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
          </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation</papertitle>
          <br>
          <a href="https://shuzhaoxie.github.io/">Shuzhao Xie</a>,
          Weixiang Zhang, 
          <a href="https://www.chentang.cc">Chen Tang</a>,
          <strong>Yunpeng Bai</strong>,
          Rongwei Lu,
          Shijia Ge,
          <a href="http://zwang.inflexionlab.org">Zhi Wang</a>
          <br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2024,
          <br>
          <a href="data/24-eccv-mesongs.pdf">[PDF]</a>
          <a href="https://github.com/ShuzhaoXie/MesonGS">[Code]</a> 
          <a href="https://shuzhaoxie.github.io/mesongs/">[Project]</a> 
          <br>
          <p>In this paper, we propose MesonGS, a codec for post-training compression of 3D Gaussians. 
            Initially, we introduce a measurement criterion that considers both view-dependent and view-independent factors to assess the impact of each 
            Gaussian point on the rendering output, enabling the removal of insignificant points. Subsequently, we decrease the 
            entropy of attributes through two transformations that complement subsequent entropy coding techniques to enhance the file compression rate. </p>
      </td>
  </tr> 

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
              <source src='images/HFA-GP.mp4'>
  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>High-Fidelity Facial Avatar Reconstruction From Monocular Video With Generative Priors</papertitle>
              <br>
              <strong>Yunpeng Bai</strong>, <a href="https://sites.google.com/site/yanbofan0124/">Yanbo Fan</a>, <a href="https://xuanwangvc.github.io/">Xuan Wang</a>, <a href="https://yzhang2016.github.io/">Yong Zhang</a>, <a href="https://mrtornado24.github.io/">Jingxiang Sun</a>, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2023,
      <!-- <strong><font color="#FF0000">Oral Presentation</font></strong> -->
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.html">[PDF]</a>
              <a href="https://github.com/bbaaii/HFA-GP">[Code]</a>
              <!-- <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a> -->
      <!-- <a href="images/discoface.txt">[BibTeX]</a> -->
              <br>
              <p>In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. </p>
          </td>
      </tr>
          




        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/TextIR.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>TextIR: A Simple Framework for Text-based Editable Image Restoration</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, Cairong Wang, <a href="https://shuzhaoxie.github.io/">Shuzhao Xie</a>, <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a>, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>, <a href="http://zwang.inflexionlab.org">Zhi Wang</a>
                <br>
                <em>In submission to IEEE Transactions on Visualization and Computer Graphics (TVCG)</em> 2024,
                <br>
                <a href="https://arxiv.org/abs/2302.14736">[PDF]</a>
                <!-- <a href="https://github.com/bbaaii/DreamDiffusion">[Code]</a> 
                <br> -->
                <p>In this work, we design an effective framework that allows the user to control the restoration process of degraded images with text descriptions. We use the text-image feature compatibility of the CLIP to alleviate the difficulty of fusing text and image features. Our framework can be used for various image restoration tasks, including image inpainting, image super-resolution, and image colorization.</p>
            </td>
        </tr> 






          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <img src='images/NOFA.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NOFA: NeRF-based One-shot Facial Avatar Reconstruction</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=UOE8-qsAAAAJ&hl=zh-CN">Wangbo Yu</a>, <a href="https://sites.google.com/site/yanbofan0124/">Yanbo Fan</a>, <a href="https://yzhang2016.github.io/">Yong Zhang</a>, <a href="https://xuanwangvc.github.io/">Xuan Wang</a>, <a href="https://feiiyin.github.io/">Fei Yin</a>, <strong>Yunpeng Bai</strong>, <a href="https://yanpei.me/">Yan-Pei Cao</a>, <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>, <a href="https://scholar.google.com/citations?user=T-HaQ84AAAAJ&hl=en">Yang Wu</a>, Zhongqian Sun, <a href="https://sites.google.com/site/baoyuanwu2015/home">Baoyuan Wu</a>
              <br>
              <em>SIGGRAPH (Conference Track)</em> 2023,
      <!-- <strong><font color="#FF0000">Oral Presentation</font></strong> -->
              <br>
              <a href="https://arxiv.org/pdf/2307.03441.pdf">[PDF]</a>
              <a href="https://bbaaii.github.io/">[Code]</a>
              <!-- <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a> -->
      <!-- <a href="images/discoface.txt">[BibTeX]</a> -->
              <br>
              <p>We propose a one-shot 3D facial avatar reconstruction framework, which only requires a single source image to reconstruct high-fidelity 3D facial avatar, by leveraging the rich generative prior of 3D GAN and developing an efficient encoder-decoder network. </p>
          </td>
      </tr>  



      <tr></tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" >
              <img src='images/SEAM.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
          </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <papertitle>SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization</papertitle>
          <br>
          <a href="https://www.chentang.cc">Chen Tang</a>, <a href="https://scholar.google.com/citations?user=Zq7-VzwAAAAJ&hl=en">Kai Ouyang</a>, <a href="https://zenghaochai.com/">Zenghao Chai</a>, <strong>Yunpeng Bai</strong>, <a href="https://mengyuan404.github.io/">Yuan Meng</a>, <a href="http://zwang.inflexionlab.org">Zhi Wang</a>, <a href="https://scholar.google.com/citations?user=7t2jzpgAAAAJ&hl=en">Wenwu Zhu</a>
          <br>
          <em>ACM International Conference on Multimedia (ACM MM)</em> 2023,
          <br>
          <a href="https://arxiv.org/pdf/2302.06845.pdf">[PDF]</a>
          <!-- <a href="https://github.com/bbaaii/DreamDiffusion">[Code]</a> 
          <br> -->
          <p>In this paper, we propose to search the effective MPQ policy by using a small proxy dataset for the model trained on a large-scale one. It breaks the routine that requires a consistent dataset at model training and MPQ policy search time, which can improve the MPQ searching efficiency significantly.</p>
      </td>
      </tr> 

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/PS-NeRV.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>PS-NeRV: Patch-wise Stylized Neural Representations for Videos</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a>, Cairong Wang, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>
                <br>
                <em>IEEE International Conference on Image Processing (ICIP)</em> 2023,
				<!-- <strong><font color="#FF0000">Oral Presentation</font></strong> -->
                <br>
                <a href="https://arxiv.org/abs/2208.03742">[PDF]</a>
                <!-- <a href="https://github.com/microsoft/DiscoFaceGAN">[Code]</a> -->
				<!-- <a href="images/discoface.txt">[BibTeX]</a> -->
                <br>
                <p>We study how to represent a video with implicit neural representations (INRs). Classical INRs methods generally utilize MLPs to map input coordinates to output pixels. While some recent works have tried to directly reconstruct the whole image with CNNs. However, we argue that both the above pixel-wise and image-wise strategies are not favorable to video data. Instead, we propose a patch-wise solution, PS-NeRV, which represents videos as a function of patches and the corresponding patch coordinate. It naturally inherits the advantages of image-wise methods, and achieves excellent reconstruction performance with fast decoding speed. </p>
            </td>
        </tr>

        <!-- <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/style.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Improving the Latent Space of Image Style Transfer</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, Cairong Wang, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>
                <br>
                <em>arXiv</em> 2022,
                <br>
                <a href="https://arxiv.org/pdf/2205.12135.pdf">[PDF]</a> -->
                <!-- <a href="https://github.com/sicxu/Deep3dPortrait">[Code]</a> -->
				<!-- <a href="images/deep3dportrait.txt">[BibTeX]</a> -->
                <!-- <br>
                <p>Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task.</p>
            </td>
        </tr> -->

        
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                  <img src='images/eccv2022.jpg' style="width:100%;max-width:100%; position: absolute;top: -5%">
		<!-- <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/accurate3d.mp4'>
		</video> -->
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Semantic-Sparse Colorization Network for Deep Exemplar-based Colorization</papertitle>
                <br>
                <strong>Yunpeng Bai</strong>, <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ"
                target="_blank">Chao Dong</a>, <a href="https://zenghaochai.com/">Zenghao Chai</a>, Andong Wang, <a href="https://xuzhengzhuo.com/">Zhengzhuo Xu</a>, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2022, 
                <!-- <strong><font color="#FF0000">Best Paper Award</font></strong> -->
                <br>
                <a href="https://arxiv.org/pdf/2112.01335.pdf">[PDF]</a>
                <a href="https://github.com/bbaaii/SSC-Net">[Code]</a>
				<!-- <a href="images/accurate3d.txt">[BibTeX]</a> -->
                <br>
                <p>We propose Semantic-Sparse Colorization Network (SSCN) to transfer both the global image style and detailed semantic-related colors to the gray-scale image in a coarse-to-fine manner. Our network can perfectly balance the global and local colors while alleviating the ambiguous matching problem.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                <img src='images/cms-lstm.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning</papertitle>
                <br>
                <a href="https://zenghaochai.com/">Zenghao Chai</a>, <a href="https://xuzhengzhuo.com/">Zhengzhuo Xu</a>, <strong>Yunpeng Bai</strong>, <a href="https://scholar.google.com/citations?user=t4et8FEAAAAJ&hl=en">Zhihui Lin</a>, <a href="https://www.sigs.tsinghua.edu.cn/yc2_en/main.htm">Chun Yuan</a>
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME)</em> 2022,
                <br>
                <a href="https://arxiv.org/pdf/2102.03586.pdf">[PDF]</a>
                <a href="https://github.com/czh-98/CMS-LSTM">[Code]</a>
				<!-- <a href="images/dif_net.txt">[BibTeX]</a> -->
                <br>
                <p>To tackle the increasing ambiguity during forecasting, we design CMS-LSTM to focus on context correlations and multi-scale spatiotemporal flow with details on fine-grained locals, containing two elaborate designed blocks: Context Embedding (CE) and Spatiotemporal Expression (SE) blocks. CE is designed for abundant context interactions, while SE focuses on multi-scale spatiotemporal expression in hidden states.</p>
            </td>
        </tr>
        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src="images/TRNet.png" style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Latent Tangent Space Representation for Normal Estimation</papertitle>
              <br>
              <a href="https://jjcao.github.io/">Junjie Cao</a>, Hairui Zhu, <strong>Yunpeng Bai</strong>, Jun Zhou, <a href="https://jspan.github.io/">Jinshan Pan</a>, <a href="http://faculty.dlut.edu.cn/ZhixunSu/en/index.htm">Zhixun Su</a>   
              <br>
              <em>IEEE Transactions on Industrial Electronics (TIE)</em>, 2022, 69(1), 921-929, 
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9339852">[PDF]</a>
              <a href="https://github.com/jjcao/TRNET19">[Code]</a>
              <br>
              <p>We propose a simple deep network to estimate the normal vector based on a latent tangent space representation learned in the network. We call the network tangent represent learning network (TRNet). For each query point, the tangent space representation is a set of latent points spanning the tangent plane of it. The representation is generated using only the coordinates of its neighbors and regularized by a differentiable random sample consensus like component, which makes TRNet more compact and effective for normal estimation. </p>
            </td>
          </tr>
        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://jonbarron.info/">Jon Barron</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
